# Data Processing Service Test Suite

This directory contains comprehensive tests for the Data Processing Microservice, covering unit tests, integration tests, and endpoint tests.

## Test Structure

```
tests/
├── __init__.py                 # Test package initialization
├── conftest.py                # Shared fixtures and configuration
├── test_config_manager.py     # Unit tests for configuration management
├── test_database_manager.py   # Unit tests for database operations
├── test_integration.py        # Integration tests for complete workflows
├── test_endpoints.py          # Endpoint tests for microservice API
├── test_runner.py             # Test runner and reporting utilities
├── pytest.ini                # Pytest configuration
├── requirements-test.txt      # Testing dependencies
└── README.md                 # This file
```

## Test Categories

### 1. Unit Tests
- **ConfigManager**: Configuration loading, validation, environment variable handling
- **DatabaseManager**: Database operations, schema creation, data insertion
- **Extractors**: Individual extractor functionality and data validation
- **Utilities**: Field detection, ID normalization, data cleaning

### 2. Integration Tests
- **Complete Workflow**: End-to-end data processing from Excel to database
- **File Validation**: Input file validation and error handling
- **Data Extraction**: Multi-file data extraction and relationship mapping
- **Database Population**: Complete database schema creation and population
- **Error Recovery**: Error handling and recovery scenarios
- **Large Datasets**: Performance testing with large data volumes

### 3. Endpoint Tests
- **Health Checks**: Service health and status endpoints
- **Data Processing**: Processing workflow through API endpoints
- **Configuration**: Configuration management through endpoints
- **Validation**: File validation through API
- **Metrics**: Service metrics and monitoring endpoints
- **Error Handling**: API error handling and recovery

## Running Tests

### Prerequisites

Install testing dependencies:
```bash
pip install -r requirements-test.txt
```

### Basic Test Execution

```bash
# Run all tests
pytest

# Run specific test categories
pytest tests/test_config_manager.py          # Unit tests only
pytest tests/test_integration.py            # Integration tests only
pytest tests/test_endpoints.py              # Endpoint tests only

# Run with verbose output
pytest -v

# Run specific test methods
pytest tests/test_config_manager.py::TestConfigManager::test_load_config_success
```

### Using the Test Runner

```bash
# Run all test suites with reporting
python tests/test_runner.py --type all --verbose

# Run specific test types
python tests/test_runner.py --type unit
python tests/test_runner.py --type integration
python tests/test_runner.py --type endpoint

# Run with coverage report
python tests/test_runner.py --coverage

# Run performance tests
python tests/test_runner.py --performance
```

### Advanced Test Options

```bash
# Run tests in parallel
pytest -n auto

# Run tests with coverage
pytest --cov=. --cov-report=html --cov-report=term-missing

# Run only fast tests (exclude slow tests)
pytest -m "not slow"

# Run tests with specific markers
pytest -m "unit"
pytest -m "integration"
pytest -m "endpoint"

# Generate HTML test report
pytest --html=test_report.html --self-contained-html
```

## Test Configuration

### Environment Variables

Tests use the following environment variables:
- `TEST_DATA_DIR`: Directory for test data files
- `TEST_OUTPUT_DIR`: Directory for test output
- `TEST_CONFIG_FILE`: Test configuration file
- `TEST_LOG_LEVEL`: Logging level for tests

### Test Data

Tests use mock data generated by fixtures:
- **Sample Risk Data**: 3 sample risk records
- **Sample Control Data**: 3 sample control records  
- **Sample Question Data**: 3 sample question records
- **Large Datasets**: Configurable large datasets for performance testing

### Test Fixtures

Key fixtures available:
- `temp_dir`: Temporary directory for test files
- `sample_config`: Sample configuration file
- `sample_risks_data`: Sample risk data DataFrame
- `sample_controls_data`: Sample control data DataFrame
- `sample_questions_data`: Sample question data DataFrame
- `mock_excel_files`: Mock Excel files for testing
- `config_manager`: ConfigManager instance
- `database_manager`: DatabaseManager instance
- `data_processor`: DataProcessor instance

## Test Coverage

### Current Coverage Goals
- **Unit Tests**: 90%+ coverage for core components
- **Integration Tests**: 80%+ coverage for workflow scenarios
- **Endpoint Tests**: 100% coverage for API endpoints

### Coverage Reports

Generate coverage reports:
```bash
# HTML coverage report
pytest --cov=. --cov-report=html
open htmlcov/index.html

# Terminal coverage report
pytest --cov=. --cov-report=term-missing

# XML coverage report (for CI/CD)
pytest --cov=. --cov-report=xml
```

## Continuous Integration

### GitHub Actions Example

```yaml
name: Data Processing Service Tests

on: [push, pull_request]

jobs:
  test:
    runs-on: ubuntu-latest
    strategy:
      matrix:
        python-version: [3.9, 3.10, 3.11]
    
    steps:
    - uses: actions/checkout@v3
    - name: Set up Python ${{ matrix.python-version }}
      uses: actions/setup-python@v3
      with:
        python-version: ${{ matrix.python-version }}
    
    - name: Install dependencies
      run: |
        pip install -r requirements.txt
        pip install -r requirements-test.txt
    
    - name: Run unit tests
      run: pytest tests/test_config_manager.py tests/test_database_manager.py --cov=. --cov-report=xml
    
    - name: Run integration tests
      run: pytest tests/test_integration.py
    
    - name: Run endpoint tests
      run: pytest tests/test_endpoints.py
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
```

## Test Development Guidelines

### Writing New Tests

1. **Follow naming conventions**:
   - Test files: `test_*.py`
   - Test classes: `Test*`
   - Test methods: `test_*`

2. **Use appropriate markers**:
   ```python
   @pytest.mark.unit
   def test_config_loading():
       pass
   
   @pytest.mark.integration
   def test_complete_workflow():
       pass
   
   @pytest.mark.endpoint
   def test_api_endpoint():
       pass
   ```

3. **Use fixtures for common setup**:
   ```python
   def test_something(temp_dir, sample_config):
       # Use fixtures for setup
       pass
   ```

4. **Test both success and failure cases**:
   ```python
   def test_success_case():
       # Test happy path
       pass
   
   def test_error_case():
       # Test error handling
       with pytest.raises(ValueError):
           # Code that should raise error
           pass
   ```

### Test Data Management

- Use fixtures for reusable test data
- Generate test data dynamically when possible
- Clean up test data after tests
- Use temporary directories for file-based tests

### Mocking Guidelines

- Mock external dependencies (file system, network calls)
- Use `unittest.mock.patch` for patching
- Mock at the appropriate level (not too high, not too low)
- Verify mock calls when important

## Troubleshooting

### Common Issues

1. **Import Errors**:
   - Ensure test directory is in Python path
   - Check relative imports in test files

2. **File Not Found Errors**:
   - Verify test data files exist
   - Check file paths in fixtures

3. **Database Errors**:
   - Ensure test database is created properly
   - Check database permissions

4. **Timeout Errors**:
   - Increase timeout for slow tests
   - Use `@pytest.mark.slow` for long-running tests

### Debug Mode

Run tests in debug mode:
```bash
pytest --pdb  # Drop into debugger on failures
pytest -s     # Don't capture output
pytest -vv    # Extra verbose output
```

## Performance Testing

### Benchmark Tests

Run performance benchmarks:
```bash
pytest --benchmark-only tests/test_integration.py::TestDataProcessingIntegration::test_large_dataset_processing
```

### Memory Profiling

Profile memory usage:
```bash
pytest --profile tests/test_integration.py
```

## Contributing

When adding new tests:

1. Follow existing patterns and conventions
2. Add appropriate test markers
3. Update this documentation if needed
4. Ensure tests pass in CI/CD environment
5. Add test data fixtures if needed
6. Update coverage goals if applicable

## Resources

- [Pytest Documentation](https://docs.pytest.org/)
- [Pytest Fixtures](https://docs.pytest.org/en/stable/fixture.html)
- [Pytest Markers](https://docs.pytest.org/en/stable/mark.html)
- [Coverage.py Documentation](https://coverage.readthedocs.io/)
